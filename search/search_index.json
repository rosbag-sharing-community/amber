{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AMBER: Automated annotation and Multimodal Bag Extraction for Robotics","text":"<p>Amber is a ROS2 friendly ML tools. Your rosbag2 become dataset!</p>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>Prepare rosbag2 with mcap format.</li> <li>Prepare task description yaml file.</li> <li>Enjoy your ML life with Robots!</li> </ol>"},{"location":"#how-to-setup","title":"How to setup?","text":""},{"location":"#check-your-os-is-suppoted-platform","title":"Check your OS is suppoted platform?","text":"<p>This tool is only support ubuntu 22.04. Please install ubuntu 22.04 in your local machine first.</p>"},{"location":"#install-dependencies","title":"Install Dependencies","text":""},{"location":"#poetry","title":"Poetry","text":"<p>Setup environment and dependencies in python. Please follow this documentation.</p> <p>Warning</p> <p>Developer and github actions are tested under poetry 1.5.1</p>"},{"location":"#docker","title":"Docker","text":"<p>Some automation tools are executed inside docker. Please follow this documentation.</p> <p>Notion</p> <p>Developer use docker version 23.0.5</p>"},{"location":"#nvidia-driver-and-nvidia-dockeroptional","title":"Nvidia driver and nvidia docker(Optional)","text":"<p>Some automation tools support cuda. If you want to use gpu, please install nvidia driver and nvidia docker.</p>"},{"location":"#google-testoptional","title":"Google test(Optional)","text":"<p>Google test is a used for testing C++ code inside amber. It is optional and it is not required for building amber in your local machine.</p> <pre><code>sudo apt update &amp; sudo apt install -y googletest\n</code></pre>"},{"location":"tips/","title":"Tips","text":""},{"location":"tips/#i-do-not-have-rosbag2-but-i-have-rosbag-ros1","title":"I do not have rosbag2, but I have rosbag (ROS1)","text":"<p>This tool can convert your rosbag into rosbag2 very easily.</p>"},{"location":"tips/#my-rosbag-is-not-a-mcap-format","title":"My rosbag is not a mcap format.","text":"<p>You can use <code>ros2 bag convert</code> command. First, you prepare compress.yaml like below.</p> <pre><code>output_bags:\n  - uri: rosbag\n    storage_id: mcap\n    compression_mode: message\n    compression_format: zstd\n    all: true\n    compression_queue_size: 0\n    compression_threads: 0\n</code></pre> <pre><code>ros2 bag convert -i (PATH_TO_ROSBAG) -o conversion.yaml\n</code></pre>"},{"location":"automation/automation/","title":"Automation","text":"<p>Automation helps you to enjoy your ML life. Automation tools can be used with two ways, use with CLI and use with Python API</p>"},{"location":"automation/automation/#cli-tools","title":"CLI tools","text":"<p>All of the automation tools can be use <code>amber automation</code> command. If you want to check the help text, please type <code>amber automation --help</code></p>"},{"location":"automation/automation/#python-api","title":"Python API","text":"<p>All of the automation tools have Python classes. If you want to know detail, please read detatiled documentations.</p>"},{"location":"automation/automation/#support-status","title":"Support status","text":"Name Docker Support (CPU) Docker Support(CUDA) Native Support CUDA Support(Native) Huggingface Support DeticImageLabaler ClipImageAnnotationFilter NeRF 3D Reconstruction"},{"location":"automation/automation/#docker-support","title":"Docker Support","text":"<p>Support automation algorithm inside docker.</p>"},{"location":"automation/automation/#native-support","title":"Native Support","text":"<p>Support automation algorithm in native environment.</p>"},{"location":"automation/automation/#hugging-face-support","title":"Hugging face Support","text":"<p>Support running automation algorithum on hugging face.</p>"},{"location":"automation/automation/#cuda-support","title":"CUDA Support","text":"<p>Support cuda for accelerating automation.</p>"},{"location":"automation/clip_image_annotation_filter/","title":"Clip image annotation filter","text":"<p>Warning</p> <p>This feature is just experimental and very low quality.</p> <p>You can query object from annotated rosbags by prompt by using OpenAI CLIP.</p>"},{"location":"automation/clip_image_annotation_filter/#use-with-cli","title":"Use with CLI","text":"<pre><code>amber automation clip_image_annotation_filter tests/automation/clip_image_annotation_filter.yaml tests/rosbag/ford_with_annotation/read_images_and_bounding_box.yaml tests/rosbag/ford_with_annotation/bounding_box.mcap output.mcap\n</code></pre> <p>Task description yaml for the detic_image_labaler is here.</p> <pre><code>target_objects: [\"passenger car.\"] # Target object you want to find.\n\n# If the width of the bounding box overs min_width or the height of the bounding box overs min_height of the bounding box, it recoganize as candidate bounding box.\nmin_width: 30 # min width of the object in pixels.\nmin_height: 30 # min height of the object in pixels.\nmin_area: 50 # min area of the object in pixels.\n\n# Classification method, you can choose from two method.\n# You can choose from clip_with_lvis_and_custom_vocabulary, consider_annotation_with_bert\nclassify_method: consider_annotation_with_bert\n# Configuration parameter for consider_annotation_with_bert, this value was used when you choose `consider_annotation_with_bert`\nconsider_annotation_with_bert_config:\n  positive_nagative_ratio: 1.0 # Ratio of the cosine similarity of positive and negative prompt. Negative prompt is \"Not a photo of $target_object\".\n  min_clip_cosine_similarity: 0.25 # Minimum values of cosine similarity with clip text/image embeddings.\n  min_clip_cosine_similarity_with_berf: 0.3 # Minimum values of cosine similarity with clip text embeddings and image enbeddings consider prompt similarity using bert.\n</code></pre>"},{"location":"automation/clip_image_annotation_filter/#algorithms","title":"Algorithms","text":""},{"location":"automation/clip_image_annotation_filter/#consider_annotation_with_bert","title":"consider_annotation_with_bert","text":"<p>python script of the <code>consider_annotation_with_bert</code> is below.</p> <pre><code># Pure clip cosine similarity.\nclip_similarity = cosine_similarity(\n    clip_embeddings / torch.sum(clip_embeddings),\n    self.text_embeddings[target_object][0],\n)\n# Clip cosine similarity considering bert embeddings.\npositive = cosine_similarity(\n    clip_embeddings / torch.sum(clip_embeddings)\n    + annotation_text_embeddings\n    / torch.sum(annotation_text_embeddings)\n    * self.text_encoder.cosine_similarity(\n        bounding_box.object_class, target_object\n    ),\n    self.text_embeddings[target_object][0],\n)\n# Pure clip cosine similarity with negative prompt.\nnegative = cosine_similarity(\n    clip_embeddings / torch.sum(clip_embeddings),\n    self.text_embeddings[target_object][1],\n)\n</code></pre>"},{"location":"automation/clip_image_annotation_filter/#clip_with_lvis_and_custom_vocabulary","title":"clip_with_lvis_and_custom_vocabulary","text":"<p>Embed all object categories in lvis and append custom vocabulary to the text embedding tensor. Then, find most nearest category.</p> <pre><code>if self.lvis_text_embeddings == None:\n    with torch.no_grad():\n        # Make text embeddings from all lvis objects.\n        self.lvis_text_embeddings = self.model.encode_text(\n            tokenize(self.lvis_prompts).to(self.device)\n        )\nprompts: List[str] = []\n# Construct a prompt for target object.\nfor text in texts:\n    prompts.append(\"A photo of a \" + text)\nwith torch.no_grad():\n    text_embeddings = torch.cat(\n        [\n            self.lvis_text_embeddings,\n            self.model.encode_text(tokenize(prompts).to(self.device)),\n        ],\n        dim=0,\n    )\n    image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n    text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n    # Find nearby category. This code is based on OpenAI official implementation.\n    # See also https://github.com/openai/CLIP/tree/main#zero-shot-prediction\n    similarity = (\n        image_embeddings.to(torch.float32) @ text_embeddings.to(torch.float32).T\n    ).softmax(dim=-1)\n    values, indices = similarity.topk(1)\n    for value, index in zip(values, indices):\n        if index &lt; len(self.lvis_classes):\n            return None\n        else:\n            return (texts[index - len(self.lvis_classes)], value.item())\n</code></pre>"},{"location":"automation/clip_image_annotation_filter/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nfilter = ClipImageAnnotationFilter(str(current_path / \"automation\" / \"clip_image_annotation_filter.yaml\"))\ndataset = ImagesAndAnnotationsDataset(\n    str(current_path / \"rosbag\" / \"ford_with_annotation\" / \"bounding_box.mcap\"),\n    str(current_path / \"rosbag\" / \"ford_with_annotation\" / \"read_images_and_bounding_box.yaml\"),\n)\nannotations = filter.inference(dataset)\n</code></pre>"},{"location":"automation/detic_image_labaler/","title":"Detic image labaler","text":"<p>Detic is a deep learning algorithum developed by facebook research. This tool generate annotation data by using detic. Detic can classify 21k classes. This tools are running onnx converted detic models with opset=16 in this repository. Thank you for ailia-models developers.</p>"},{"location":"automation/detic_image_labaler/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>This sample command is written with the assumption that it will be executed in the root directory of the amber package.</p> <pre><code>amber automation detic_image_labeler tests/automation/detic_image_labeler.yaml tests/rosbag/ford/read_image.yaml tests/rosbag/ford/ford.mcap output.mcap\n</code></pre> <p>Task description yaml for the detic_image_labaler is here.</p> <pre><code>confidence_threshold: 0.5      # If the confidence overs the threshold, detic determines the object are exists.\nvideo_output_path: output.mp4  # Relative path to the visualization result.\nvocabulary: \"lvis\"             # Vocabulary of detic, you can choose from \"lvis\" and \"imagenet_21k\"\nmodel_type: \"SwinB_896_4x\"     # Model type of detic, you can choose from \"SwinB_896_4x\" and \"R50_640_4x\"\n</code></pre> <p>After executing this command, <code>output.mp4</code> movie file was generated.</p>"},{"location":"automation/detic_image_labaler/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nlabeler = DeticImageLabeler(str(current_path / \"automation\" / \"detic_image_labeler.yaml\"))\ndataset = ImagesDataset(\n    str(current_path / \"rosbag\" / \"ford\" / \"ford.mcap\"),\n    str(current_path / \"rosbag\" / \"ford\" / \"read_image.yaml\"),\n)\nlabeler.inference(dataset)\n</code></pre> <p><code>detic_image_labeler.yaml</code> and <code>read_image.yaml</code> are exactly same when you use detic_image_labaler with CLI.</p> <p>After executing this command, <code>output.mp4</code> movie file was generated.</p>"},{"location":"automation/nerf_3d_reconstruction/","title":"NeRF 3D Reconstruction","text":"<p>NeRF is a deep learning rendering algorithm called Neural radiance fields. AMBER works with nerf-studio, where various NeRF algorithms are implemented, to learn NeRF from image data recorded in rosbag.</p> <p>Special thanks to nerfstudio contributers.</p> <p>Warning</p> <p>Limitation : NeRF 3D reconstruction feature is just developed and not well-used. This feature currently supports nerfacto and poisson surface reconstruction method.</p>"},{"location":"automation/nerf_3d_reconstruction/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>Currently, generated mesh and pointcloud cannot be write into rosbag. So, final argument <code>hoge.mcap</code> does not work.</p> <pre><code>amber automation nerf tests/nerf_3d_reconstruction.yaml tests/rosbag/soccer_goal/read_image.yaml tests/rosbag/soccer_goal/ hoge.mcap\n</code></pre>"},{"location":"automation/nerf_3d_reconstruction/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nlabeler = Nerf3DReconstruction(str(current_path / \"automation\" / \"nerf_3d_reconstruction.yaml\"))\ndataset = ImagesDataset(\n    str(current_path / \"rosbag\" / \"soccer_goal\"),\n    str(current_path / \"rosbag\" / \"soccer_goal\", \"read_image.yaml\"),\n)\nlabeler.inference(dataset)\n</code></pre> <p><code>nerf_3d_reconstruction.yaml</code> and <code>read_image_ford.yaml</code> are exactly same when you use detic_image_labaler with CLI.</p>"},{"location":"automation/nerf_3d_reconstruction/#how-to-see-the-training-result","title":"How to see the training result.","text":"<p>When the training and exporting 3D mesh finished. Output like below was generated.</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udf89 Training Finished \ud83c\udf89 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                        \u2577                                                                                            \u2502\n\u2502   Config File          \u2502 /workspace/outputs/checkpoints/camera_poses/nerfacto/2023-06-10_100233/config.yml          \u2502\n\u2502   Checkpoint Directory \u2502 /workspace/outputs/checkpoints/camera_poses/nerfacto/2023-06-10_100233/nerfstudio_models   \u2502\n\u2502                        \u2575                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nPrinting profiling stats, from longest to shortest duration in seconds\n\nVanillaPipeline.get_average_eval_image_metrics: 11.2157\n\nVanillaPipeline.get_eval_image_metrics_and_images: 0.4561\n\nVanillaPipeline.get_eval_loss_dict: 0.0392\n\nTrainer.train_iteration: 0.0260\n\nVanillaPipeline.get_train_loss_dict: 0.0170\n\nTrainer.eval_iteration: 0.0014\n\nfind: paths must precede expression: `|'\n\nfind: paths must precede expression: `|'\n\nSuccessfully copied 420.6MB to /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e/outputs\nArtifacts are outputed under /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e/outputs\nIf you want to check the trained result, please type commands below :\ndocker run -it --rm --gpus all -p 7007:7007             -v /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e:/workspace dromni/nerfstudio:0.3.1/bin/bash -c \"find -name config.yml | xargs -I {} ns-viewer --load-config {}\"\n</code></pre> <p>If you want to see the NeRF result, please type final commands shown in the result like below.\u3000\u3000</p> <pre><code>docker run -it --rm --gpus all -p 7007:7007             -v /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e:/workspace dromni/nerfstudio:0.3.1/bin/bash -c \"find -name config.yml | xargs -I {} ns-viewer --load-config {}\"\n</code></pre> <p>Open url showed in console. Then, you can see result like below.</p>"},{"location":"import/import/","title":"Import","text":"<p>Import helps you to import another data format integrate to amber. Import tools can be used with two ways, use with CLI and use with Python API.</p>"},{"location":"import/import/#cli-tools","title":"CLI tools","text":"<p>All of the import tools can be use <code>amber import</code> command. If you want to check the help text, please type <code>amber import --help</code></p>"},{"location":"import/import/#python-api","title":"Python API","text":"<p>All of the automation tools have Python classes. If you want to know detail, please read detatiled documentations.</p>"},{"location":"import/import/#support-status","title":"Support status","text":"Name Filetype Image Audio Video mp4 tf tf2_amber.TransformStamped python object"},{"location":"import/tf/","title":"TF","text":"<p>TF import tool can write amber_mcap.tf2_amber.TransformStamped python object to mcap file as <code>tf2_msgs/msg/TFMessage</code> in ROS 2.</p>"},{"location":"import/tf/#how-to-use","title":"How to use","text":"<pre><code>config = TfImporterConfig()\nconfig.rosbag_path = \"output.mcap\" # Path to output mcap rosbag file.\nimporter = TfImporter(config)\nsample_data = TransformStamped()\nsample_data.child_frame_id = \"base_link\"\nsample_data.header.frame_id = \"map\"\nsample_data.header.stamp.nanosec = 0\nsample_data.header.stamp.sec = 0\nsample_data.transform.translation.x = 1\nimporter.write(sample_data)\nsample_data.header.stamp.sec = 10\nimporter.write(sample_data)\nimporter.finish()\n</code></pre> <p>Then, run <code>ros2 bag info output.mcap</code></p> <pre><code>closing.\n\nFiles:             tests/output.mcap\nBag size:          4.0 KiB\nStorage id:        mcap\nDuration:          10.000000000s\nStart:             Jan  1 1970 09:00:00.000000000 (0.000000000)\nEnd:               Jan  1 1970 09:00:10.000000000 (10.000000000)\nMessages:          2\nTopic information: Topic: /tf | Type: tf2_msgs/msg/TFMessage | Count: 2 | Serialization Format: cdr\n</code></pre>"},{"location":"import/tf/#view-rosbag-data-in-foxglove","title":"View rosbag data in foxglove","text":""},{"location":"import/video/","title":"Video","text":"<p>Video import tool can convert video files into rosbag. Currently, only <code>*.mp4</code> file types are supported.</p>"},{"location":"import/video/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>This sample command is written with the assumption that it will be executed in the root directory of the amber package.</p> <pre><code>amber import video tests/video/soccer_goal.mp4 tests/video_importer.yaml\n</code></pre> <p>If you want to know the option, please run <code>amber import video -h</code> command.</p> <p>example of the inporter config is here.</p> <pre><code>topic_name: /camera/image_raw\nrosbag_path: output.mcap\n</code></pre>"},{"location":"import/video/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nimporter = VideoImporter(\n    str(current_path / \"video\" / \"soccer_goal.mp4\"),\n    str(current_path / \"video_importer.yaml\"),\n)\nimporter.write()\n</code></pre>"},{"location":"sample_applications/image_search/","title":"Image Search","text":"<p>This application compares the image embedding calculated from the images in the rosbag with the text embedding calculated from the prompts entered by the user using various Vision&amp;Language models, and presents the closest result as the search result.</p> <p>In order to use this application, please setup amber with </p> <pre><code>poetry install --with apps\n</code></pre> <p>This application uses qdrant, a type of vector search engine, for searching.</p> <p></p> <p>1- shows the processing steps up to the point where the embedding of the image is calculated and it is registered in qdrant. 2- indicates the point where prompt input is accepted and displayed.</p>"},{"location":"sample_applications/image_search/#run-application-with-ford-dataset","title":"Run application with ford dataset.","text":"<p>Warning</p> <p>This sample command is written with the assumption that it will be executed in the root directory of the amber package.</p> <pre><code>python3 amber/apps/image_search.py --rosbag_directory tests/rosbag/ford/ --sampling_duration=0.1\n</code></pre> <p>If it works correctly, the following message is displayed.</p> <pre><code>Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n</code></pre> <p>Please access this URL as soon as the message is confirmed.</p> <p>You can search for images by entering a prompt in gradio's UI as shown in the video below.</p>"},{"location":"sample_applications/sample_applications/","title":"Sample Applications","text":"<p>This page shows a sample implementation of an application that can be developed using amber.</p>"},{"location":"sample_applications/sample_applications/#image-search","title":"Image Search","text":"<p>This application supports text search against images with multiple Vison-Language models. For more information, click here.</p>"},{"location":"use_rosbag_as_dataset/how_to_use/","title":"How to use?","text":""},{"location":"use_rosbag_as_dataset/how_to_use/#prepare-rosbag-data","title":"Prepare rosbag data","text":"<p>Warning</p> <p>Currently, some limitations are exists. rosbag2 should be mcap format. only zstd message compression supports.</p>"},{"location":"use_rosbag_as_dataset/how_to_use/#use-rosbag-as-dataset","title":"Use rosbag as dataset","text":"<p>Rosbag includes multimodal data, such as image/pointcloud/audio/etc... So, you need to describe what data you want to extract from rosbag. In order to specify this, write yaml setting file.</p> <p>If you want to know wahat types of task <code>amber</code> supports, please check below.</p> Name Image 2D Annotation PointCloud ImagesDataset ImagesAndAnnotation PointCLoudsDataset"},{"location":"use_rosbag_as_dataset/how_to_use/#sampling-data-from-dataset","title":"Sampling data from dataset.","text":"<p>The rosbags record a variety of time series data, but when inputting this data into machine learning, you may want to sample based on conditions rather than inputting everything. Amber implements samplers for rosbags to make it easier to perform time series processing.</p> <p>Currently, only timestamp sampler supports. If you want to know details, see documentation.</p>"},{"location":"use_rosbag_as_dataset/read_images/","title":"Read images","text":"<p>ImagesDataset Class provides only image data. Example task description yaml file is here.</p> <pre><code>image_topics:\n  - topic_name: /wamv/sensors/cameras/front_left_camera_sensor/image_raw\n  - topic_name: /wamv/sensors/cameras/front_right_camera_sensor/image_raw\ncompressed: true\n</code></pre> <pre><code>from amber_mcap.dataset.images_and_annotations_dataset import (\n    ImagesDataset,\n    ReadImagesConfig,\n)\n\ndataset = ImagesDataset(\n  \"(path to rosbag .mcap file)\",\n  ReadImagesConfig.from_yaml_paht(\"(path to rosbag yaml description file)\"))\n</code></pre>"},{"location":"use_rosbag_as_dataset/read_images_and_annotations/","title":"Read images and annotations","text":"<p>ImagesAndAnnotationsDataset Class provides image and annotations. Example task description yaml file is here.</p> <pre><code>image_topics:\n  - topic_name: /image_front_left\nannotation_topic: /detic_image_labeler/annotation\ncompressed: false\n</code></pre> <pre><code>from amber_mcap.dataset.images_and_annotations_dataset import (\n    ImagesAndAnnotationsDataset,\n    ReadImagesAndAnnotationsConfig,\n)\n\ndataset = ImagesAndAnnotationsDataset(\n  \"(path to rosbag .mcap file)\",\n  ReadImagesAndAnnotationsConfig.from_yaml_file(\"(path to rosbag yaml description file)\"))\n</code></pre>"},{"location":"use_rosbag_as_dataset/read_images_and_annotations/#supported-annotation-type","title":"Supported annotation type","text":"Name Support status Remarks Bounding Box Polygon Mask"},{"location":"use_rosbag_as_dataset/read_pointclouds/","title":"Read images","text":"<p>PointcloudDataset Class provides only image data. Example task description yaml file is here.</p> <pre><code>pointcloud_topics:\n  - topic_name: /wamv/sensors/lidars/lidar_wamv_sensor/points\ncompressed: false\n</code></pre> <pre><code>from amber_mcap.dataset.pointcloud_dataset import PointcloudDataset, ReadPointCloudConfig\n\ndataset = PointcloudDataset(\n  \"(path to rosbag .mcap file)\",\n  ReadPointCloudConfig.from_yaml_file(\"(path to rosbag yaml description file)\"))\n</code></pre>"},{"location":"use_rosbag_as_dataset/read_tf/","title":"Read tf","text":"<p>ImagesDataset Class provides only transform data. Example task description yaml file is here.</p> <pre><code>topic_name: \"/tf\"\nstatic_tf_topic_name: \"/tf_static\"\ntarget_frame: body\nsource_frame: map\n</code></pre> <pre><code>from amber_mcap.dataset.tf_dataset import TfDataset, ReadTfTopicConfig\n\ncurrent_path = Path(os.path.dirname(os.path.realpath(__file__)))\ndataset = TfDataset(\n    str(current_path / \"rosbag\" / \"ford\" / \"ford.mcap\"),\n    ReadTfTopicConfig.from_yaml_file(\n        str(current_path / \"rosbag\" / \"ford\" / \"read_tf.yaml\")\n    ),\n)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\ncount = 0\nfor i_batch, sample_batched in enumerate(dataloader):\n    for sample in sample_batched:\n        count = count + 1\n</code></pre>"},{"location":"use_rosbag_as_dataset/read_tf/#output-tensor","title":"Output tensor","text":"<p>The output tensor has 7 dimensions.</p> <pre><code>torch.Tensor([\n    transform.translation.x, \n    transform.translation.y, \n    transform.translation.z\n    transform.rotation.x,\n    transform.rotation.y\n    transform.rotation.z\n    transform.rotation.w])\n</code></pre>"},{"location":"visualize/clip_image_embedding/","title":"Visualize CLIP Image embedding","text":""},{"location":"visualize/clip_image_embedding/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>This sample command is written with the assumption that it will be executed in the root directory of the amber package.</p> <pre><code>amber visualize image_embedding tests/visualization/clip_image_visualization.yaml tests/rosbag/ford_with_annotation/read_images_and_bounding_box.yaml tests/rosbag/ford_with_annotation/bounding_box.mcap\ntensorboard --host 0.0.0.0 --port 6006 --logdir runs\n</code></pre> <p>After executing tensorboard, access http://0.0.0.0:6006#projector by your browser.</p> <p>In tensorboard, you can see the embedding space of the CLIP.</p>"},{"location":"visualize/clip_image_embedding/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nvisualization = ClipEmbeddingsVisualization(\n    str(current_path / \"visualization\" / \"clip_image_visualization.yaml\")\n)\ndataset = ImagesAndAnnotationsDataset(\n    str(current_path / \"rosbag\" / \"ford_with_annotation\" / \"bounding_box.mcap\"),\n    str(\n        current_path\n        / \"rosbag\"\n        / \"ford_with_annotation\"\n        / \"read_images_and_bounding_box.yaml\"\n    ),\n)\nvisualization.visualize(dataset)\n</code></pre>"},{"location":"visualize/visualize/","title":"Visualize","text":"<p>Visualization tools helps you to enjoy your ML life. Visualization tools can be used with two ways, use with CLI and use with Python API</p>"},{"location":"visualize/visualize/#cli-tools","title":"CLI tools","text":"<p>All of the visualization tools can be use <code>amber visualize</code> command. If you want to check the help text, please type <code>amber visualize --help</code></p>"},{"location":"visualize/visualize/#python-api","title":"Python API","text":"<p>All of the visualization tools have Python classes. If you want to know detail, please read detatiled documentations.</p>"},{"location":"visualize/visualize/#supported-status","title":"Supported Status","text":"Name CPU Support GPU Support CLIP Image Embedding"}]}