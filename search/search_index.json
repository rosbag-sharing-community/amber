{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AMBER: Automated annotation and Multimodal Bag Extraction for Robotics","text":"<p>Amber is a ROS2 friendly ML tools. Your rosbag2 become dataset!</p>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>Prepare rosbag2 with mcap format.</li> <li>Prepare task description yaml file.</li> <li>Enjyo your ML life with Robots!</li> </ol>"},{"location":"#how-to-setup","title":"How to setup?","text":""},{"location":"#check-your-os-is-suppoted-platform","title":"Check your OS is suppoted platform?","text":"<p>This tool is only support ubuntu 22.04. Please install ubuntu 22.04 in your local machine first.</p>"},{"location":"#install-dependencies","title":"Install Dependencies","text":""},{"location":"#poetry","title":"Poetry","text":"<p>Setup environment and dependencies in python. Please follow this documentation.</p> <p>Warning</p> <p>Developer and github actions are tested under poetry 1.5.1</p>"},{"location":"#docker","title":"Docker","text":"<p>Some automation tools are executed inside docker. Please follow this documentation.</p> <p>Notion</p> <p>Developer use docker version 23.0.5</p>"},{"location":"#nvidia-driver-and-nvidia-dockeroptional","title":"Nvidia driver and nvidia docker(Optional)","text":"<p>Some automation tools support cuda. If you want to use gpu, please install nvidia driver and nvidia docker.</p>"},{"location":"#google-testoptional","title":"Google test(Optional)","text":"<p>Google test is a used for testing C++ code inside amber. It is optional and it is not required for building amber in your local machine.</p> <pre><code>sudo apt update &amp; sudo apt install -y googletest\n</code></pre>"},{"location":"tips/","title":"Tips","text":""},{"location":"tips/#i-do-not-have-rosbag2-but-i-have-rosbag-ros1","title":"I do not have rosbag2, but I have rosbag (ROS1)","text":"<p>This tool can convert your rosbag into rosbag2 very easily.</p>"},{"location":"tips/#my-rosbag-is-not-a-mcap-format","title":"My rosbag is not a mcap format.","text":"<p>You can use <code>ros2 bag convert</code> command. First, you prepare compress.yaml like below.</p> <pre><code>output_bags:\n- uri: rosbag\nstorage_id: mcap\ncompression_mode: message\ncompression_format: zstd\nall: true\ncompression_queue_size: 0\ncompression_threads: 0\n</code></pre> <pre><code>ros2 bag convert -i (PATH_TO_ROSBAG) -o conversion.yaml\n</code></pre>"},{"location":"automation/automation/","title":"Automation","text":"<p>Automation helps you to enjoy your ML life. Automation tools can be used with two ways, use with CLI and use with Python API</p>"},{"location":"automation/automation/#cli-tools","title":"CLI tools","text":"<p>All of the automation tools can be use <code>amber automation</code> command. If you want to check the help text, please type <code>amber automation --help</code></p>"},{"location":"automation/automation/#python-api","title":"Python API","text":"<p>All of the automation tools have Python classes. If you want to know detail, please read detatiled documentations.</p>"},{"location":"automation/automation/#support-status","title":"Support status","text":"Name Docker Support (CPU) Docker Support(CUDA) Native Support CUDA Support(Native) Huggingface Support DeticImageLabaler NeRF 3D Reconstruction"},{"location":"automation/automation/#docker-support","title":"Docker Support","text":"<p>Support automation algorithm inside docker.</p>"},{"location":"automation/automation/#native-support","title":"Native Support","text":"<p>Support automation algorithm in native environment.</p>"},{"location":"automation/automation/#hugging-face-support","title":"Hugging face Support","text":"<p>Support running automation algorithum on hugging face.</p>"},{"location":"automation/automation/#cuda-support","title":"CUDA Support","text":"<p>Support cuda for accelerating automation.</p>"},{"location":"automation/detic_image_labaler/","title":"Detic image labaler","text":"<p>Detic is a deep learning algorithum developed by facebook research. This tool generate annotation data by using detic. Detic can classify 21k classes. This tools are running onnx converted detic models with opset=16 in this repository. Thank you for ailia-models developers.</p>"},{"location":"automation/detic_image_labaler/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>This sample command is written with the assumption that it will be executed in the root directory of the amber package.</p> <pre><code>amber automation detic_image_labeler tests/automation/detic_image_labeler.yaml tests/rosbag/ford/read_image.yaml tests/rosbag/ford/ford.mcap output.mcap\n</code></pre> <p>Task description yaml for the detic_image_labaler is here.</p> <pre><code>confidence_threshold: 0.5      # If the confidence overs the threshold, detic determines the object are exists.\nvideo_output_path: output.mp4  # Relative path to the visualization result.\nvocabulary: \"lvis\"             # Vocabulary of detic, you can choose from \"lvis\" and \"imagenet_21k\"\nmodel_type: \"SwinB_896_4x\"     # Model type of detic, you can choose from \"SwinB_896_4x\" and \"R50_640_4x\"\n</code></pre> <p>After executing this command, <code>output.mp4</code> movie file was generated.</p>"},{"location":"automation/detic_image_labaler/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nlabeler = DeticImageLabeler(str(current_path / \"automation\" / \"detic_image_labeler.yaml\"))\ndataset = ImagesDataset(\n    str(current_path / \"rosbag\" / \"ford\" / \"ford.mcap\"),\n    str(current_path / \"rosbag\" / \"ford\" / \"read_image.yaml\"),\n)\nlabeler.inference(dataset)\n</code></pre> <p><code>detic_image_labeler.yaml</code> and <code>read_image.yaml</code> are exactly same when you use detic_image_labaler with CLI.</p> <p>After executing this command, <code>output.mp4</code> movie file was generated.</p>"},{"location":"automation/nerf_3d_reconstruction/","title":"NeRF 3D Reconstruction","text":"<p>NeRF is a deep learning rendering algorithm called Neural radiance fields. AMBER works with nerf-studio, where various NeRF algorithms are implemented, to learn NeRF from image data recorded in rosbag.</p> <p>Special thanks to nerfstudio contributers.</p> <p>Warning</p> <p>Limitation : NeRF 3D reconstruction feature is just developed and not well-used. This feature currently supports nerfacto and poisson surface reconstruction method.</p>"},{"location":"automation/nerf_3d_reconstruction/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>Currently, generated mesh and pointcloud cannot be write into rosbag. So, final argument <code>hoge.mcap</code> does not work.</p> <pre><code>amber automation nerf tests/nerf_3d_reconstruction.yaml tests/rosbag/soccer_goal/read_image.yaml tests/rosbag/soccer_goal/ hoge.mcap\n</code></pre>"},{"location":"automation/nerf_3d_reconstruction/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nlabeler = Nerf3DReconstruction(str(current_path / \"automation\" / \"nerf_3d_reconstruction.yaml\"))\ndataset = ImagesDataset(\n    str(current_path / \"rosbag\" / \"soccer_goal\"),\n    str(current_path / \"rosbag\" / \"soccer_goal\", \"read_image.yaml\"),\n)\nlabeler.inference(dataset)\n</code></pre> <p><code>nerf_3d_reconstruction.yaml</code> and <code>read_image_ford.yaml</code> are exactly same when you use detic_image_labaler with CLI.</p>"},{"location":"automation/nerf_3d_reconstruction/#how-to-see-the-training-result","title":"How to see the training result.","text":"<p>When the training and exporting 3D mesh finished. Output like below was generated.</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udf89 Training Finished \ud83c\udf89 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                        \u2577                                                                                            \u2502\n\u2502   Config File          \u2502 /workspace/outputs/checkpoints/camera_poses/nerfacto/2023-06-10_100233/config.yml          \u2502\n\u2502   Checkpoint Directory \u2502 /workspace/outputs/checkpoints/camera_poses/nerfacto/2023-06-10_100233/nerfstudio_models   \u2502\n\u2502                        \u2575                                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nPrinting profiling stats, from longest to shortest duration in seconds\n\nVanillaPipeline.get_average_eval_image_metrics: 11.2157\n\nVanillaPipeline.get_eval_image_metrics_and_images: 0.4561\n\nVanillaPipeline.get_eval_loss_dict: 0.0392\n\nTrainer.train_iteration: 0.0260\n\nVanillaPipeline.get_train_loss_dict: 0.0170\n\nTrainer.eval_iteration: 0.0014\n\nfind: paths must precede expression: `|'\n\nfind: paths must precede expression: `|'\n\nSuccessfully copied 420.6MB to /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e/outputs\nArtifacts are outputed under /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e/outputs\nIf you want to check the trained result, please type commands below :\ndocker run -it --rm --gpus all -p 7007:7007             -v /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e:/workspace dromni/nerfstudio:0.3.1/bin/bash -c \"find -name config.yml | xargs -I {} ns-viewer --load-config {}\"\n</code></pre> <p>If you want to see the NeRF result, please type final commands shown in the result like below.\u3000\u3000</p> <pre><code>docker run -it --rm --gpus all -p 7007:7007             -v /tmp/nerf_3d_reconstruction/6d1873fd-fa94-4c1a-a592-b4fd5cbc927e:/workspace dromni/nerfstudio:0.3.1/bin/bash -c \"find -name config.yml | xargs -I {} ns-viewer --load-config {}\"\n</code></pre> <p>Open url showed in console. Then, you can see result like below.</p>"},{"location":"import/import/","title":"Import","text":"<p>Import helps you to import another data format integrate to amber. Import tools can be used with two ways, use with CLI and use with Python API.</p>"},{"location":"import/import/#cli-tools","title":"CLI tools","text":"<p>All of the import tools can be use <code>amber import</code> command. If you want to check the help text, please type <code>amber import --help</code></p>"},{"location":"import/import/#python-api","title":"Python API","text":"<p>All of the automation tools have Python classes. If you want to know detail, please read detatiled documentations.</p>"},{"location":"import/import/#support-status","title":"Support status","text":"Name Filetype Image Audio Video mp4"},{"location":"import/video/","title":"Video","text":"<p>Video import tool can convert video files into rosbag. Currently, only <code>*.mp4</code> file types are supported.</p>"},{"location":"import/video/#use-with-cli","title":"Use with CLI","text":"<p>Warning</p> <p>This sample command is written with the assumption that it will be executed in the root directory of the amber package.</p> <pre><code>amber import video tests/video/soccer_goal.mp4 tests/video_importer.yaml\n</code></pre> <p>If you want to know the option, please run <code>amber import video -h</code> command.</p> <p>example of the inporter config is here.</p> <pre><code>topic_name: /camera/image_raw\nrosbag_path: output.mcap\n</code></pre>"},{"location":"import/video/#use-with-python-api","title":"Use with Python API","text":"<pre><code>current_path = Path(os.path.dirname(os.path.realpath(__file__)))\nimporter = VideoImporter(\n    str(current_path / \"video\" / \"soccer_goal.mp4\"),\n    str(current_path / \"video_importer.yaml\"),\n)\nimporter.write()\n</code></pre>"},{"location":"use_rosbag_as_dataset/how_to_use/","title":"How to use?","text":""},{"location":"use_rosbag_as_dataset/how_to_use/#prepare-rosbag-data","title":"Prepare rosbag data","text":"<p>Warning</p> <p>Currently, some limitations are exists. rosbag2 should be mcap format. only zstd message compression supports.</p>"},{"location":"use_rosbag_as_dataset/how_to_use/#use-rosbag-as-dataset","title":"Use rosbag as dataset","text":"<p>Rosbag includes multimodal data, such as image/pointcloud/audio/etc... So, you need to describe what data you want to extract from rosbag. In order to specify this, write yaml setting file.</p> <p>If you want to know wahat types of task <code>amber</code> supports, please check below.</p> Name Image 2D Annotation PointCloud ImagesDataset ImagesAndAnnotation PointCLoudsDataset"},{"location":"use_rosbag_as_dataset/read_images/","title":"Read images","text":"<p>ImagesDataset Class provides only image data. Example task description yaml file is here.</p> <pre><code>image_topics:\n- topic_name: /wamv/sensors/cameras/front_left_camera_sensor/image_raw\n- topic_name: /wamv/sensors/cameras/front_right_camera_sensor/image_raw\ncompressed: true\n</code></pre> <pre><code>dataset = ImagesDataset(\"(path to rosbag .mcap file)\", \"(path to rosbag yaml description file)\")\n</code></pre>"},{"location":"use_rosbag_as_dataset/read_images_and_annotations/","title":"Read images and annotations","text":"<p>ImagesAndAnnotationsDataset Class provides image and annotations. Example task description yaml file is here.</p> <pre><code>image_topics:\n- topic_name: /image_front_left\nannotation_topic: /detic_image_labeler/annotation\ncompressed: false\n</code></pre> <pre><code>dataset = ImagesAndAnnotationsDataset(\"(path to rosbag .mcap file)\", \"(path to rosbag yaml description file)\")\n</code></pre>"},{"location":"use_rosbag_as_dataset/read_images_and_annotations/#supported-annotation-type","title":"Supported annotation type","text":"Name Support status Remarks Bounding Box Polygon Mask"},{"location":"use_rosbag_as_dataset/read_pointclouds/","title":"Read images","text":"<p>PointcloudDataset Class provides only image data. Example task description yaml file is here.</p> <pre><code>pointcloud_topics:\n- topic_name: /wamv/sensors/lidars/lidar_wamv_sensor/points\ncompressed: false\n</code></pre> <pre><code>dataset = PointcloudDataset(\"(path to rosbag .mcap file)\", \"(path to rosbag yaml description file)\")\n</code></pre>"}]}